# cl_lora-cifar100-b5-5-20

dataset: &dataset cifar100
data_root: /data/lqx/cifar100
init_cls_num: &init_cls_num 5
inc_cls_num: &inc_cls_num 5
total_cls_num: &total_cls_num 100
task_num: &task_num 20
image_size: &image_size 224
epoch: &epoch 30

dataset: *dataset
init_cls_num: *init_cls_num
inc_cls_num: *inc_cls_num
total_cls_num: *total_cls_num
task_num: *task_num

epoch: *epoch
val_per_epoch: *epoch

batch_size: 64

setting: task-agnostic 

testing_times: 1
testing_per_task: False

seed: 1993

train_trfms:
  - RandomResizedCrop:
      size: *image_size
      scale: [0.05, 1.0]
      ratio: [0.75, 1.3333]
  - RandomHorizontalFlip: {}
  - ToTensor: {}

test_trfms: 
  - Resize:
      size: 256
      interpolation: 3 # BICUBIC
  - CenterCrop:
      size: [*image_size, *image_size]
  - ToTensor: {}

optimizer:
  name: SGD
  kwargs:
    lr: 0.03
    momentum: 0.9
    weight_decay: 0.0001

lr_scheduler:
  name: CosineAnnealingLR
  kwargs:
    T_max: *epoch
    eta_min: 0

backbone:
  name: vit_cl_lora
  kwargs:
    pretrained: True
    model_name : vit_base_patch16_224_in21k
    attn_layer: MultiHeadAttention_CL_LoRA
    transformer_layer: Transformer_CL_LoRA
    lora_rank: 8
    norm_layer_eps: 1e-6

classifier:
  name: CL_LoRA
  kwargs:
    init_cls_num: *init_cls_num
    inc_cls_num: *inc_cls_num
    task_num: *task_num
    embd_dim: 768